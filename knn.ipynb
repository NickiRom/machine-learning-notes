{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#KNN\n",
    "- non-parametric\n",
    "- lazy learner: uses all training data every time a prediction is made (as opposed to SVM, in which everything but support vectors can be discarded)\n",
    "- for binary/multi-label classification or regression\n",
    "\n",
    "- Training data consists of vectors (in feature space) with labels\n",
    "\n",
    "###KNN for Density estimation\n",
    "- since it is non-parametric, can estimate density for arbitrary distributions\n",
    "- place a hypercube at point x and expand until k points are encompassed\n",
    "- calculate density:\n",
    "    \\begin{equation*}\n",
    "    P(x) = \\frac{k}{NV} = \\frac{k}{N \\times c_D \\times R_{k}^{D}(x)}\n",
    "    \\end{equation*}\n",
    "    - k is the number of points encompassed    \n",
    "    - N is the total number of points in feature space\n",
    "    - V is the volume of the hypercube\n",
    "    - $c_D$ is the volume of the unit sphere in D dimensions\n",
    "    - $R_k(x)$ is the distance between the estimation point and the kth-closest neighbor (aka the radius of the hypersphere)\n",
    "    <img src=\"images/knn_hypercube.png\" height=\"500\" width=\"300\">\n",
    "    \n",
    "###Choosing k\n",
    "- Rule of thumb: choose a k of $\\sqrt{N}$ for a set of N samples\n",
    "- if the number of classes is 2, k is usually odd (so that a majority vote wins)\n",
    "- can validate choice of k using cross-validation or bootstrapping\n",
    "\n",
    "<img src=\"images/knn_performance_by_N_and_k.png\" height=\"500\" width=\"300\">\n",
    "\n",
    "###Distance Metrics\n",
    "***Note: *** All features must be standardized to be between 0 and 1, so one feature does not dominate simply because it has a bigger range\n",
    "\n",
    "**Euclidean:** \n",
    "\\begin{equation*}\n",
    "\\sqrt{\\sum_{i=1}^k(x_i-y_i)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "**Manhattan:** \n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^k\\mid(x_i-y_i)\\mid\n",
    "\\end{equation*}\n",
    "\n",
    "**Minkowski:** \n",
    "\\begin{equation*}\n",
    "\\left(\\sum_{i=1}^k \\left( \\mid x_i - y_i \\mid \\right)^q \\right)^{1/q}\n",
    "\\end{equation*}\n",
    "\n",
    "**Hamming:** \n",
    "- for categorical variables\n",
    "\n",
    "\\begin{equation*}\n",
    "D_H = \\sum_{i=1}^k \\mid x_i - y_i \\mid\n",
    "\\\\\n",
    "x=y \\to D=0 \\\\ x\\neq y \\to D=1\n",
    "\\end{equation*}\n",
    "\n",
    "###Improve resource utilization\n",
    "**How to reduce storage costs**\n",
    "- Can store vectors representing each cluster's center of mass\n",
    "- edited KNN: \n",
    "    - remove misclassified training examples\n",
    "    - remove correctly classified training examples (all that are left are the boundary examples\n",
    "- select prototype examples\n",
    "\n",
    "**Speed up NN search**\n",
    "- Bucketing: feature space is divided into cells; each cell has an address and a list of data points.  Cells are examined in order of increasing distance from query point\n",
    "- k-d trees: for each query point, algo descends the tree to find the most similar data points\n",
    "    - each node is associated with a hyper-rectangle and a hyper-plane orthogonal to one of the coordinateaxes\n",
    "    - the hyper-plane splits the hyper-rectangle into two parts, leaving two child nodes\n",
    "    - partitioning continues until # of data points in each hyper-rectangle is below a threshold\n",
    "    - partitions become finer in data point-dense regions\n",
    "    <img src=\"images/k-d_trees.png\" height=\"300\" width=\"500\">\n",
    "\n",
    "### Cons of KNN\n",
    "- sensitive to noisy features: must normalize each feature to N(0,1)\n",
    "- sensitive to high dimensionality if only a few features carry classification information: must remove low-information features\n",
    "\n",
    "### Feature selection using KNN\n",
    "**Performance weighting (wrappers): ** find a set of weights iteratively\n",
    "- use statistical re-sampling technique to estimate accuracy of feature subsets\n",
    "- tends to give better performance\n",
    "\n",
    "**Preset bias (filters): ** use a predetermined function to measure mutual information between each feature and the class label (executes quickly)\n",
    "- independent of any learning algorithm\n",
    "- more computationally efficient\n",
    "\n",
    "**Relief algorithms**\n",
    "- estimates quality of features by ability to distinguish between instances of different classes that are close in feature space\n",
    "- do not assume conditional independence of features (so interactions are ok)\n",
    "\n",
    "    Steps:\n",
    "    1. randomly sample an instance\n",
    "    2. locate the nearest neighbors of the same and opposite classes\n",
    "    3. compare feature values of same and opposite classes\n",
    "    4. update feature importance accordingly\n",
    "    \n",
    "**ReliefF extension**\n",
    "- better for multi-class and noisy datasets\n",
    "- averages feature values over k nearest neighbors for same and opposite classes\n",
    "- for multi-class, contributions are weighted by prior probabilities\n",
    "\n",
    "**Computational efficiency improvements**\n",
    "- FSSMC (Feature Selection via Supervised Model Construction): replaces training set instances with cluster centroids\n",
    "\n",
    "[Feature selection Methods](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.103.117&rep=rep1&type=pdf)\n",
    "\n",
    "###Types of algorithms\n",
    "1. **BallTree**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
