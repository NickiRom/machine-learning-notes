{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing airflow\n",
    "- source activate venv\n",
    "- pip install airflow\n",
    "- brew install mysql\n",
    "\n",
    "##### Set Airflow's home\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "##### install from pypi using pip\n",
    "pip install airflow\n",
    "\n",
    "##### create config\n",
    "airflow version\n",
    "\n",
    "##### initialize the database\n",
    "airflow initdb\n",
    "\n",
    "##### start the web server, default port is 8080\n",
    "airflow webserver -p 8080\n",
    "\n",
    "(note: If -d debug flag is desired, you'll need an SSL certificate.  Can create using openSSL)\n",
    "\n",
    "\n",
    "##### go to UI\n",
    "go to localhost:8080\n",
    "\n",
    "##### see config files:\n",
    "(venv) nickirom: ~ > cd ~/airflow/\n",
    "\n",
    "##### Notes about scaling airflow\n",
    "- airflow.cfg is where you can change location folders to s3\n",
    "- check if your airflow_home, airflow_dags_folder, and airflow_logs_folder have appropriate paths\n",
    "- executor = SequentialExecutor, but can use others to do things like shard your DAGs (to have one executor always do ML, another do data munching)\n",
    "- dags_are_paused_at_creation: allows you to not start DAGs right away when you publish them\n",
    "\n",
    "### Dag factory\n",
    "- universal_exports in airflow\n",
    "- an oop approach to create a DAG instance\n",
    "- then add tasks\n",
    "- walk through config folder\n",
    "- and export dag = DagFactory(...).build_dag()\n",
    "- locals()[export_dag.dag_id] = export_dag\n",
    "\n",
    "### Monitoring and Alerts\n",
    "- enable EmailOperator or SlackOperator for monitoring\n",
    "- SLA feature lets you know when jobs are not completing on time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands\n",
    "\n",
    "##### airflow render\n",
    "- allows you to show formatted sql code with updated variables\n",
    "\n",
    "##### run\n",
    "- what the scheduler uses to run tasks.  Can be used to run one-off tasks\n",
    "\n",
    "##### run and check on status:\n",
    "**(venv) nickirom: ~ >** airflow run tutorial print_date 2017-04-24\n",
    "\n",
    "*[2017-04-26 11:06:55,531] {__init__.py:57} INFO - Using executor SequentialExecutor\n",
    "Sending to executor.\n",
    "[2017-04-26 11:06:56,732] {__init__.py:57} INFO - Using executor SequentialExecutor\n",
    "Logging into: /Users/nickirom/cape/airflow/airflow-workshop-dataengconf-sf-2017/logs/tutorial/print_date/2017-04-24T00:00:00*\n",
    "\n",
    "**(venv) nickirom: ~ >** airflow task_state tutorial print_date 2017-04-24\n",
    "\n",
    "*[2017-04-26 11:07:30,584] {__init__.py:57} INFO - Using executor SequentialExecutor\n",
    "[2017-04-26 11:07:30,973] {models.py:167} INFO - Filling up the DagBag from /Users/nickirom/cape/airflow/airflow-workshop-dataengconf-sf-2017/example_dags\n",
    "success*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a new DB connection\n",
    "- Admin >> connections >> create tab\n",
    "- to check if connection is good, Data_Profiling >> Ad Hoc Query\n",
    "\n",
    "### import into sqlite3\n",
    "        brew install sqlite\n",
    "        (venv) nickirom: ~/airflow/airflow-workshop-dataengconf-sf-2017/data > sqlite3 babynames.sqlite3\n",
    "        >> SQLite version 3.13.0 2016-05-18 10:57:30\n",
    "        >> Enter \".help\" for usage hints.\n",
    "        sqlite> .mode csv\n",
    "        sqlite> .import babynamesbystate.csv babynames\n",
    "\n",
    "- then in connections, host = git repo /data/babynames.sqlite3\n",
    "\n",
    "### set up DAG\n",
    "    dag = DAG(\n",
    "        'DATAU302_example_dag',\n",
    "        default_args=default_args,\n",
    "        description=\"This will show up in the DAG view in the web UI\",\n",
    "        schedule_interval=timedelta(days=1))   # This is a daily DAG.\n",
    "        \n",
    "- first arg is name\n",
    "    \n",
    "### add to DAG \n",
    "        from airflow.operators.sqlite_operator import SqliteOperator\n",
    "        \n",
    "        t4 = SqliteOperator('sqlite', sql='SELECT state, COUNT(1) FROM babynames GROUP BY state', sqlite_conn_id='babynames', dag=dag)\n",
    "\n",
    "        t1.set_upstream(t0)\n",
    "        t2.set_upstream(t1)\n",
    "        t3.set_upstream(t1)\n",
    "        t4.set_upstream(t3)\n",
    "\n",
    "### debug\n",
    "1. remove any pyc\n",
    "        (venv) nickirom: ~/airflow/airflow-workshop-dataengconf-sf-2017/example_dags/example_dag > rm tutorial_example_dag.pyc\n",
    "2. compile in python\n",
    "        (venv) nickirom: ~/airflow/airflow-workshop-dataengconf-sf-2017/example_dags/example_dag > python sqlite_example_dag.py\n",
    "3. make sure your airflow.cfg file points to the right dags folder:\n",
    "        dags_folder = /Users/nickirom/airflow/airflow-workshop-dataengconf-sf-2017/example_dags\n",
    "4. check for dag\n",
    "        airflow list_dags\n",
    "  \n",
    "### Now list tasks\n",
    "        (venv) nickirom: ~/airflow/airflow-workshop-dataengconf-sf-2017/example_dags/example_dag > airflow list_tasks sqlite_example_dag\n",
    "        [2017-04-26 12:08:25,224] {__init__.py:57} INFO - Using executor SequentialExecutor\n",
    "        [2017-04-26 12:08:25,602] {models.py:167} INFO - Filling up the DagBag from /Users/nickirom/airflow/airflow-workshop-dataengconf-sf-2017/example_dags\n",
    "        print_date\n",
    "        show_ds\n",
    "        sqlite\n",
    "        templated_task\n",
    "        wait_a_second\n",
    "        \n",
    "        \n",
    "### Run task\n",
    "        (venv) nickirom: ~/airflow/airflow-workshop-dataengconf-sf-2017/example_dags/example_dag > airflow test sqlite_example_dag sqlite 2017-04-26\n",
    "        [2017-04-26 12:10:19,938] {__init__.py:57} INFO - Using executor SequentialExecutor\n",
    "        [2017-04-26 12:10:20,314] {models.py:167} INFO - Filling up the DagBag from /Users/nickirom/airflow/airflow-workshop-dataengconf-sf-2017/example_dags\n",
    "        [2017-04-26 12:10:25,709] {models.py:1126} INFO - Dependencies all met for <TaskInstance: sqlite_example_dag.sqlite 2017-04-26 00:00:00 [None]>\n",
    "        [2017-04-26 12:10:25,712] {models.py:1126} INFO - Dependencies all met for <TaskInstance: sqlite_example_dag.sqlite 2017-04-26 00:00:00 [None]>\n",
    "        [2017-04-26 12:10:25,712] {models.py:1318} INFO -\n",
    "        --------------------------------------------------------------------------------\n",
    "        Starting attempt 1 of 2\n",
    "        --------------------------------------------------------------------------------\n",
    "\n",
    "        [2017-04-26 12:10:25,713] {models.py:1342} INFO - Executing <Task(SqliteOperator): sqlite> on 2017-04-26 00:00:00\n",
    "        [2017-04-26 12:10:25,726] {sqlite_operator.py:47} INFO - Executing: SELECT state, COUNT(1) FROM babynames GROUP BY state\n",
    "        [2017-04-26 12:10:25,730] {base_hook.py:67} INFO - Using connection to: /Users/nickirom/airflow/airflow-workshop-dataengconf-sf-2017/data/babynames.sqlite3\n",
    "        [2017-04-26 12:10:25,730] {dbapi_hook.py:167} INFO - SELECT state, COUNT(1) FROM babynames GROUP BY state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices\n",
    "- make tasks idempotent (don't re-write past)\n",
    "- always try to use the operator corresponding to where the data is being processed\n",
    "    - example: if loading from s3 and processing in python, don't use s3 operator, as this ONLY gets and syncs data.  Instead, find a way (using bash to source python and s3 commands, or just using python) to load using the processing operator\n",
    "    - or write your own operators.  An example that processes data from s3: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/s3_file_transform_operator.py\n",
    "    - pull 2156 from incubator-airflow\n",
    "    \n",
    "- stay away from sub-DAGs\n",
    "- you can instead have DAGs depend on each other using sensors\n",
    "- if you need an intervention during the pipeline, create a very long or very short sensor poking at a dummy dataset.  It should fail, and the DAG only progresses when you mark that step as a success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A DAG docstring might be a good way to explain at a high level\n",
    "what problem space the DAG is looking at.\n",
    "Links to design documents, upstream dependencies etc\n",
    "are highly recommended.\n",
    "\"\"\"\n",
    "from datetime import date, datetime, timedelta\n",
    "from airflow.models import DAG  # Import the DAG class\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.sensors import TimeDeltaSensor\n",
    "\n",
    "d = date.today()\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'nickirom',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(d.year, d.month, d.day) - timedelta(days=7),\n",
    "    'email': ['nicole@capeanalytics.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'default',    # 'default' or 'silver' or 'backfill'\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='beta-dev_notebook_dag',\n",
    "    default_args=default_args,\n",
    "    description=\"This will show up in the DAG view in the web UI\",\n",
    "    schedule_interval=timedelta(days=1))   # This is a daily DAG.\n",
    "# t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "t0 = TimeDeltaSensor(\n",
    "    task_id='wait_a_second',\n",
    "    delta=timedelta(seconds=1),\n",
    "    dag=dag)\n",
    "\n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "\n",
    "\n",
    "def my_cool_function(ds=None, **kwargs):\n",
    "    print \"{}\".format(ds)\n",
    "\n",
    "\n",
    "t2 = PythonOperator(\n",
    "    task_id='show_ds',\n",
    "    python_callable=my_cool_function,\n",
    "    retries=3,\n",
    "    provide_context=True,\n",
    "    dag=dag)\n",
    "\n",
    "# Airflow uses a templating language called Jinja\n",
    "#\n",
    "\n",
    "templated_command = \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7)}}\"\n",
    "        echo \"{{ params.my_param }}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='templated_task',\n",
    "    bash_command=templated_command,\n",
    "    params={'my_param': 'This is my parameter value'},\n",
    "    dag=dag)\n",
    "\n",
    "t4 = PostgresOperator(task_id='beta-dev_notebook', \n",
    "                      sql=\"\\copy (SELECT * from development.attribute_entry_types limit 10) TO '~/cape/data_dumps/test.csv' WITH DELIMITER ',' CSV HEADER\", \n",
    "                      postgres_conn_id='beta-dev', \n",
    "                      dag=dag)\n",
    "\n",
    "t1.set_upstream(t0)\n",
    "t2.set_upstream(t1)\n",
    "t3.set_upstream(t1)\n",
    "t4.set_upstream(t3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv]",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
