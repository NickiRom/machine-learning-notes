{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of MapReduce\n",
    "\n",
    "Notes and images from [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf), by Leskovec, Rajaraman, Ullman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is MapReduce?\n",
    "Mapreduce is a programming model by which parallelized and distributed algorithms can be run on large datasets that reside on a cluster\n",
    "\n",
    "### Is MapReduce the right tool?\n",
    "**Yes, if: **\n",
    "- datasets are large\n",
    "- once generated, datasets are rarely updated in place\n",
    "- computations are being performed on a cluster\n",
    "\n",
    "### What is Hadoop?\n",
    "Hadoop is a MapReduce implementation by the Apache Foundation, which can be executed within Hadoop Distributed File Systems (HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy of a MapReduce program\n",
    "\n",
    "## Jobs\n",
    "Are composed of map tasks and reduce tasks.  There can be multiple MapReduce jobs in a program or query, e.g. if the query has a nested structure. \n",
    "\n",
    "### Execution workflow\n",
    "**Input Reader: **Splits data and assigns chunks to different map nodes\n",
    "\n",
    "**Map function: **Takes a series of key-value pairs, processes them according to a map function, and returns another series of key-value pairs.  Note that in this case, keys do not have to be unique.\n",
    "\n",
    "**Partition function: **Shuffles (shards) the Map output to the Reducer nodes.  One method is to take each key's hash and modulo by the number of Reducer nodes available.\n",
    "\n",
    "**Comparison function: **Sources the Map outputs according to the Partition function and sorts them before applying to Reducer nodes.\n",
    "\n",
    "**Reduce function: **Aggregate-level computations on one or more keys\n",
    "\n",
    "<img src='../images/mapreduce_schematic.png', width = '500', height='300'>\n",
    "\n",
    "###Nodes\n",
    "**Master Node: **Computes the necessary number of map and reduce tasks. Keeps track of the identity of each map and reduce node, as well as the status of each map and reduce task. Schedules workers as they become available for new processes, or if a node has failed.\n",
    "\n",
    "**Worker Nodes: ** Can be either a Map worker or a Reduce worker, executing the actual map or reduce tasks.\n",
    "<img src='../images/mapreduce_nodes.png', width = '500', height='300'>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
