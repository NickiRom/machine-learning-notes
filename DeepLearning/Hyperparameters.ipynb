{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General hyperparameters\n",
    "## **Layer size: **\n",
    "**definition** number of neurons in a given layber\n",
    "#### considerations\n",
    "- more neurons learn more complex problems\n",
    "- too many neurons slows or jeopardizes convergence\n",
    "- too many neurons can cause overfitting\n",
    "- chapter 6 has heuristics for number of neurons\n",
    "\n",
    "## **Magnitude**\n",
    "### gradient\n",
    "### step size\n",
    "### learning rate\n",
    "#### considerations\n",
    "- too fast causes instability\n",
    "- too slow is inefficient\n",
    "\n",
    "### momentum\n",
    "- how quickly the learning rate changes\n",
    "#### Nesterov's momentum\n",
    "#### AdaGrad\n",
    "- monotonically decreasing\n",
    "- square root of the sum of squares of history of gradient computations\n",
    "#### RMSprop\n",
    "#### AdaDelta\n",
    "- with most recent history rather than accumulating like AdaGrad\n",
    "#### ADAM\n",
    "- derives learning rates from first and second moments of gradients\n",
    "\n",
    "\n",
    "\n",
    "## **Regularization**\n",
    "#### Dropout\n",
    "- omits hidden units\n",
    "- thought to encourage each neuron to learn on its own instead of relying on other neurons\n",
    "\n",
    "#### Dropconnect\n",
    "- simply omits connections from selected neurons to change number of inputs into next layer\n",
    "\n",
    "#### L1\n",
    "- neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the “noisy” inputs\n",
    "\n",
    "#### L2\n",
    "- you modify the error function you use during training to include an additional term that adds a fraction (usually given Greek letter lower case lambda) of the sum of the squared values of the weights. So larger weight values lead to larger error, and therefore the training algorithm favors and generates small weight values.\n",
    "- intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors\n",
    "\n",
    "## **Activations**\n",
    "## **Weight initialization strategy**\n",
    "## **Loss functions**\n",
    "## **Settings for epochs during training**\n",
    "#### mini-batching\n",
    "- send more than one input vector at a time\n",
    "- more efficient- can send to GPU to parallelize computations\n",
    "\n",
    "## **Normalization scheme for input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
