{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "**Parameters**: the vector of weights learned\n",
    "\n",
    "**Layers**: a fundamental architectural unit composed of neurons with activation functions\n",
    "\n",
    "**Activation functions**: linear or nonlinear transformations to the parameter, the weights and the biases \n",
    "- Sigmoid (fallen out of favor in hidden units)\n",
    "- tanh\n",
    "- hard tanh\n",
    "- ReLU\n",
    "\n",
    "**Loss functions:**\n",
    "- Squared loss\n",
    "- Logistic Loss\n",
    "- Hinge loss\n",
    "- Negative Log Likelihood\n",
    "\n",
    "Loss functions fall into 3 categories:\n",
    "- Classification\n",
    "- Regression\n",
    "- Reconstruction\n",
    "\n",
    "**Optimization methods**:\n",
    "\n",
    "**Hyperparameters**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden layers\n",
    "## How they work\n",
    "1. accept a vector of inputs\n",
    "2. affine transformation z = W.T\\*x + b\n",
    "3. element-wise nonlinear activation function g(z)\n",
    "\n",
    "## Activation functions\n",
    "### ReLU\n",
    "        h = g(W.T*x + b)\n",
    "        where g(z) = max{0,z}\n",
    "- popular choice for hidden layers\n",
    "- not differentiable at all points, but software can choose to returne one-sided derivative\n",
    "- initialize with 0.1\n",
    "- cannot learn with gradient-based methods when activation is zero\n",
    "\n",
    "### absolute value ReLU\n",
    "- fixes alphai = -1 to get g(z) = |z|\n",
    "\n",
    "### leaky ReLU\n",
    "        when zi < 0: hi = g(z, alpha)i = max(0, zi) + alphai*min(0,zi)\n",
    "- fixes alpha to a small value like 0.01\n",
    "\n",
    "### parametric ReLU (PReLU)\n",
    "- treats alpha as a learnable parameter\n",
    "\n",
    "### Maxout units\n",
    "- instead of applying g(z) element-wise, breaks z into groups of k values\n",
    "- each maxout unit outputs the maximum element of the group\n",
    "- piecewise linear function that responds to multiple directions in the input x space\n",
    "- learns the activation function\n",
    "- has k weight vectors, needing more regularization\n",
    "- have redundancy that helps resist catastrophic forgetting\n",
    "\n",
    "### Sigmoid \n",
    "$$       g(z) = \\sigma(z) $$\n",
    "- are less popular as hidden units since they saturate\n",
    "- more common outside feed-forward networks, such as in recurrent networks, probabilistic models, autoencoders (which can't use piecewise linear activation functions)\n",
    "\n",
    "### hyperbolic tangent\n",
    "\n",
    "$$ g(z) = tanh(z) = 2\\sigma(2z)-1$$\n",
    "\n",
    "### Linear\n",
    "- if all hidden units are linear, the whole network will be linear\n",
    "- but these save parameters, and are sometimes used for that reason\n",
    "\n",
    "### Softmax\n",
    "- usually an output, sometimes a hidden unit\n",
    "- represent a probability distribution over a discrete variable with k possible values\n",
    "- only used in advanced architectures that learn to manipulate memory\n",
    "\n",
    "### Radial basis function (RBF)\n",
    "$$ h_i = \\exp{(\\frac{-1}{\\sigma^2}||W-x||^2)}$$\n",
    "        \n",
    "### Softplus\n",
    "$$ g(z) = \\log(1+\\exp{a}) $$\n",
    "- smooth version of rectifier\n",
    "- not as good as a rectifier, unintuitively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation\n",
    "\n",
    "$$ Z = W^TX+b$$\n",
    "\n",
    "We have X with dimensions $(n_\\text{image}, m)$, where n is the number of pixels and m is the number of training examples\n",
    "$$\\begin{equation*}\n",
    "\\mathbf{X} =  \\begin{vmatrix}\n",
    " &\\mathbf{item1} & \\mathbf{item2} & ... & \\mathbf{m} \\\\\n",
    "\\mathbf{x_1}& | &  | & ... & |\\\\\n",
    "\\mathbf{x_2}& X^{(1)} &  X^{(2)} & ... &  X^{(m)}\\\\\n",
    "\\mathbf{x_3}& | &  | & ... & |\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$$\n",
    "\n",
    "\n",
    "$W^T$ is of dimension $(n^l, n^{l-1})$, which for the first hidden layer looks like:\n",
    "\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\mathbf{W^T} =  \\begin{vmatrix}\n",
    " &\\mathbf{x_1} & \\mathbf{x_2} & \\mathbf{x_3} \\\\\n",
    "\\mathbf{item1}& | &  | & |\\\\\n",
    "\\mathbf{item2}& X^{(1)} &  X^{(2)} & X^{(m)}\\\\\n",
    "\\vdots& \\vdots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{m} & | &  | & | \n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$$\n",
    "\n",
    "and for the subsequent hidden layers looks like:\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\mathbf{W^T} =  \\begin{vmatrix}\n",
    " &\\mathbf{node1} & \\mathbf{node2} & \\mathbf{node3} \\\\\n",
    "\\mathbf{item1}& | &  | & |\\\\\n",
    "\\mathbf{item2}& X^{(1)} &  X^{(2)} & X^{(m)}\\\\\n",
    "\\vdots& \\vdots&\\vdots&\\vdots\\\\\n",
    "\\mathbf{m} & | &  | & | \n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$$\n",
    "\n",
    "And finally $b^{[l]}$ has dimensions $(n^l, m)$, with a single vector broadcast for each example:\n",
    "$$\\begin{equation*}\n",
    "\\mathbf{b} =  \\begin{vmatrix}\n",
    " &\\mathbf{item1} & \\mathbf{item2} & ...& \\mathbf{m} \\\\\n",
    "\\mathbf{node1}& | &  | & ...& |\\\\\n",
    "\\mathbf{node2}& b &  b & ... & b\\\\\n",
    "\\mathbf{node3} & | &  | & ...& | \n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$$\n",
    "\n",
    "So that gives Z of dimensions $(n^1, m)$\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\mathbf{Z^{[1]}} =  \\begin{vmatrix}\n",
    " &\\mathbf{item1} & \\mathbf{item2} & ... & \\mathbf{m} \\\\\n",
    "\\mathbf{node1}& | &  | & ...& |\\\\\n",
    "\\mathbf{node2}& Z^{[1](1)} &  Z^{[1](2)} & ...& Z^{[1](m)}\\\\\n",
    "\\mathbf{node3}& | &  | & ... & |\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$$\n",
    "\n",
    "Now to compute A:\n",
    "\n",
    "$$A^{[1]}=\\sigma(Z^{[1]})$$\n",
    "\n",
    "$A^{[l]}$ has the same dimensions as Z: $(n^l, m)$:\n",
    "$$\\begin{equation*}\n",
    "\\mathbf{A^{[1]}} =  \\begin{vmatrix}\n",
    " &\\mathbf{item1} & \\mathbf{item2} & ... & \\mathbf{m} \\\\\n",
    "\\mathbf{node1}& | &  | & ...& |\\\\\n",
    "\\mathbf{node2}& A^{[1](1)} &  A^{[1](2)} & ...& A^{[1](m)}\\\\\n",
    "\\mathbf{node3}& | &  | & ... & |\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "\n",
    "The cost function is calculated from the loss between $A^{[L]}$, the final output layer activation function (a.k.a. $\\widehat{y}$), and y, the target variable.  Note that we're summing over all layers 1 to n, but normalizing according to all examples in set m:\n",
    "\n",
    "$$ J(W^{[1]}, b^{[1]},..., W^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum^n_{i=1}\\mathscr{L}(\\widehat{y^{(i)}}, y)$$\n",
    "\n",
    "Where $\\mathscr{L}(\\widehat{y}, y) = -\\sum^{n_{labels}}_i y_i\\log (a_i)$\n",
    "\n",
    "# Initialization\n",
    "### For ReLU\n",
    "- set Variance of $W_i$ to $\\frac{2}{n}$\n",
    "- can be done by multiplying a random variable by the square root of the variance:\n",
    "$$ W^{[l]} = np.random.randn(shape)\\times\\sqrt{\\frac{2}{n^{l-1}}}$$\n",
    "\n",
    "### For tanh\n",
    "- use Xavier initialization (with 1 on top instead of 2):\n",
    "$$\\sqrt{\\frac{1}{n^{l-1}}}$$\n",
    "\n",
    "# Regularization\n",
    "\n",
    "\n",
    "### L1\n",
    "- sparsifies matrix by removing weights where appropriate\n",
    "\n",
    "\n",
    "### L2, aka \"weight decay\"\n",
    "- reduces weights to small numbers where appropriate\n",
    "Cost function is updated to be: \n",
    "$$ J = \\frac{1}{m}\\sum^n_{i=1}\\mathscr{L}(\\widehat{y}, y) + \\frac{\\lambda}{2m}\\sum^{L}_{l} \\text{||}W^{[L]}\\text{||}_F^2$$\n",
    "\n",
    "Where $\\text{||}W^{[L]}\\text{||}_F^2 = \\sum^{n^{[\\mathscr{l}-1]}}_{i=1} \\sum^{n^{[\\mathscr{l}]}}_{j=1} W_{ij}^{[\\mathscr{l}]}$\n",
    "\n",
    "Which gets used in backpropagation for the derivative w.r.t. W:\n",
    "\n",
    "$$dW^{[l]} = dW^{[l]} + \\frac{\\lambda}{m}W^{[l]}$$\n",
    "\n",
    "### dropout\n",
    "#### Inverted dropout is the most common\n",
    "- create a \"keep probability\" and generate random numbers with the shape of the nodes for that layer\n",
    "- you get a list of True and False, the latter nullifying that node when you multiply by the A matrix (activations)\n",
    "- in order to get a full-scale output, you need to scale up the signal from non-dropped nodes\n",
    "- so divide each activation function by the \"keep probability\" to scale up\n",
    "\n",
    "$$ g(Z)_{\\text{dropout}} = \\frac{\\text{on/off node vector}}{\\text{keep probability}}\\times g(Z)$$\n",
    "\n",
    "### dropconn\n",
    "\n",
    "### early stopping\n",
    "- Watch training and dev set error together\n",
    "- Training error will continue to decrease\n",
    "- But at some point, dev set error will start to rise due to overfitting\n",
    "- Choose snapshot at # iterations representing dev set error minimum\n",
    "\n",
    "#### Pros: \n",
    "- computationally effective\n",
    "#### Cons:\n",
    "- this doesn't allow orthogonalization:\n",
    "- want to be able to control overfitting and underfitting separately\n",
    "- optimizing cost function J (controlling underfitting) can be addressed using cost function and hyperparameters such as learning rate, momentum\n",
    "- optimizing for overfitting can be addressed using regularization, augmentation, etc.\n",
    "- instead, try L2 regularization with different values of lambda\n",
    "\n",
    "### image augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation\n",
    "\n",
    "Need to compute:\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathscr{L}(a,y)}{dA^{[L]}} = -\\frac{y}{a}+\\frac{1-y}{1-a} \\text{ for logistic regression},\\\\\n",
    "\\frac{d\\mathscr{L}(a,y)}{dZ^{[L]}} = \\frac{d\\mathscr{L}}{dA^{[L]}}\\times \\frac{dA^{[L]}}{dZ^{[L]}} = (a-y)\\times-\\frac{y}{a}+\\frac{1-y}{1-a} \\text{ for logistic regression, and }\\\\\n",
    "\\frac{d\\mathscr{L}(a,y)}{dW^{[L]}} = X \\frac{d\\mathscr{L}}{dZ^{[L]}},  \\frac{d\\mathscr{L}(a,y)}{db}=\\frac{d\\mathscr{L}(a,y)}{dZ^{[L]}}\n",
    "$$\n",
    "\n",
    "And this continues all the way back through the network.  For each layer for which $W^{[L]}$ and $b$ are computed, the weights are updated.\n",
    "\n",
    "Now to update with learning rate and L2 regularization:\n",
    "\n",
    "\n",
    "$$ W^{[L]} := W^{[L]} - \\alpha\\frac{d\\mathscr{L}(a,y)}{dW^{[L]}} +  \\frac{\\lambda}{m}W^{[L]}$$\n",
    "\n",
    "Which rearranges to $$ (1-\\frac{\\alpha \\lambda}{m})W^{[L]} - \\alpha\\frac{d\\mathscr{L}(a,y)}{dW^{[L]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch\n",
    "- split X and y into batches of equal sizes\n",
    "- # iterations per epoch is equal to the number of mini batches\n",
    "- a single pass through the training set is 1 epoch\n",
    "#### Batch gradient descent\n",
    "- when you set mini-batch size m to the entire training set\n",
    "- too long per iteration\n",
    "#### Stochastic gradient descent\n",
    "- when you set mini-batch size m = 1\n",
    "- lose speedup from vectorization\n",
    "#### Best mini-batch size\n",
    "- if small training set (m<=2000), use batch gradient descent\n",
    "- choose a power of 2\n",
    "- mini-batch must fit in CPU/GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Gradient descent\n",
    "\n",
    "### Momentum\n",
    "- with each iteration t, compute usual derivatives on current mini-batch\n",
    "- compute the exponentially weighted average:\n",
    "$$V_{dW} = \\beta V_{dW} + (1-\\beta) dW$$\n",
    "- allows derivatives over several steps to be averaged out to take larger steps in a direction that was consistent over last few steps, and smaller steps where direction changed for each step\n",
    "- think of dW as acceleration and $V_{dW}$ as velocity, with $\\beta$ being friction\n",
    "\n",
    "Therefore, updates look like:\n",
    "$$ W = W - \\alpha V_{dW}$$\n",
    "** try $\\beta$ =0.9**, which is the average over last 10 iterations\n",
    "\n",
    "\n",
    "### RMSprop\n",
    "- exponentially weighted average of the squares of the derivatives\n",
    "$$S_{dW} = \\beta S_{dW} + (1-\\beta) dW^2$$\n",
    "$$ W = W - \\alpha \\frac{dW}{\\sqrt{S_{dW}}}$$\n",
    "- add a small epsilon to the denominator square root (e.g. 10^-8) to avoid numerical blow-ups\n",
    "\n",
    "### Adam optimization algorithm\n",
    "- puts momentum and RMSprop together\n",
    "\n",
    "### Learning rate decay\n",
    "- slowly reduce learning rate as training approaches convergence\n",
    "- below, decay rate and epoch number are used:\n",
    "\n",
    "$$ \\alpha = \\frac{1}{1+r_{decay}\\times n_{epoch}}$$\n",
    "\n",
    "- or exponential decay of learning rate\n",
    "- or step functions \n",
    "- or manual decay (watching model as it trains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to tune hyperparameters\n",
    "#### most important\n",
    "- alpha\n",
    "- beta (momentum)\n",
    "- mini-batch size\n",
    "- # hidden units\n",
    "\n",
    "#### second tier\n",
    "- # layers\n",
    "- learning rate decay\n",
    "\n",
    "#### choose a sample randomly\n",
    "- don't do a grid (too many points)\n",
    "- try out a random set of points in hyperparameter space\n",
    "- use a coarse to fine scheme (zoom into a region that was working well in last sample)\n",
    "\n",
    "#### choose the right scale\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
