{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "from matplotlib import cm\n",
    "from matplotlib import figure\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "\n",
    "tfd = tf.contrib.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-05-22 21:51:46.643359. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SHAPE = [28, 28]\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\",\n",
    "                   default=0.01,\n",
    "                   help=\"Initial learning rate.\")\n",
    "flags.DEFINE_integer(\"max_steps\",\n",
    "                     default=10000,\n",
    "                     help=\"Number of training steps to run.\")\n",
    "flags.DEFINE_integer(\"latent_size\",\n",
    "                     default=16,\n",
    "                     help=\"Number of dimensions in the latent code (z).\")\n",
    "flags.DEFINE_string(\"encoder_layers\",\n",
    "                    default=\"256,128\",\n",
    "                    help=\"Comma-separated list of layer sizes for the encoder.\")\n",
    "flags.DEFINE_string(\"decoder_layers\",\n",
    "                    default=\"128,256\",\n",
    "                    help=\"Comma-separated list of layer sizes for the decoder.\")\n",
    "flags.DEFINE_string(\"activation\",\n",
    "                    default=\"elu\",\n",
    "                    help=\"Activation function for all hidden layers.\")\n",
    "flags.DEFINE_integer(\"batch_size\",\n",
    "                     default=128,\n",
    "                     help=\"Batch size. Must divide evenly into dataset sizes.\")\n",
    "flags.DEFINE_string(\"data_dir\",\n",
    "                    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"),\n",
    "                                         \"vae/data\"),\n",
    "                    help=\"Directory where data is stored (if using real data).\")\n",
    "flags.DEFINE_string(\n",
    "    \"model_dir\",\n",
    "    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"), \"vae/\"),\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "flags.DEFINE_integer(\"viz_steps\",\n",
    "                     default=500,\n",
    "                     help=\"Frequency at which save visualizations.\")\n",
    "flags.DEFINE_bool(\"fake_data\",\n",
    "                  default=False,\n",
    "                  help=\"If true, uses fake data.\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-05-22 21:52:10.166289. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "def make_encoder(images):\n",
    "  \"\"\"Build encoder which takes a batch of images and returns a latent code.\n",
    "  Args:\n",
    "    images: A `int`-like `Tensor` representing the inputs to be encoded.\n",
    "      The first dimension (axis 0) indexes batch elements; all other\n",
    "      dimensions index event elements.\n",
    "  Returns:\n",
    "    encoder: A multivariate `Normal` distribution.\n",
    "  \"\"\"\n",
    "  encoder_net = tf.keras.Sequential()\n",
    "  encoder_net.add(tf.keras.layers.Flatten())\n",
    "  for units in FLAGS.encoder_layers:\n",
    "    encoder_net.add(tf.keras.layers.Dense(units,\n",
    "                                          activation=FLAGS.activation))\n",
    "  encoder_net.add(tf.keras.layers.Dense(FLAGS.latent_size * 2,\n",
    "                                        activation=None))\n",
    "  images = tf.cast(images, dtype=tf.float32)\n",
    "  net = encoder_net(images)\n",
    "  loc = net[..., :FLAGS.latent_size]\n",
    "  scale_diag = tf.nn.softplus(net[..., FLAGS.latent_size:] + 0.5)\n",
    "  return tfd.MultivariateNormalDiag(loc=loc,\n",
    "                                    scale_diag=scale_diag,\n",
    "                                    name=\"encoder_distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-05-22 22:02:20.734549. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "def make_decoder(codes):\n",
    "  \"\"\"Build decoder which takes a batch of codes and returns generated images.\n",
    "  Args:\n",
    "    codes: A `float`-like `Tensor` containing the latent\n",
    "      vectors to be decoded. These are assumed to be rank-1, so\n",
    "      the encoding `Tensor` is rank-2 with shape `[batch_size, latent_size]`.\n",
    "  Returns:\n",
    "    decoder: A multivariate `Bernoulli` distribution.\n",
    "  \"\"\"\n",
    "  decoder_net = tf.keras.Sequential()\n",
    "  for units in FLAGS.decoder_layers:\n",
    "    decoder_net.add(tf.keras.layers.Dense(units,\n",
    "                                          activation=FLAGS.activation))\n",
    "  decoder_net.add(tf.keras.layers.Dense(np.prod(IMAGE_SHAPE),\n",
    "                                        activation=None))\n",
    "  net = decoder_net(codes)\n",
    "  new_shape = tf.concat([tf.shape(net)[:-1], IMAGE_SHAPE], axis=0)\n",
    "  logits = tf.reshape(net, shape=new_shape)\n",
    "  return tfd.Independent(tfd.Bernoulli(logits=logits),\n",
    "                         reinterpreted_batch_ndims=len(IMAGE_SHAPE),\n",
    "                         name=\"decoder_distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-05-22 22:02:41.712298. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    }
   ],
   "source": [
    "def make_prior():\n",
    "  \"\"\"Build prior distribution over latent codes.\n",
    "  Returns:\n",
    "    prior: A multivariate standard `Normal` distribution.\n",
    "  \"\"\"\n",
    "  return tfd.MultivariateNormalDiag(scale_diag=tf.ones(FLAGS.latent_size),\n",
    "                                    name=\"prior_distribution\")\n",
    "\n",
    "\n",
    "def make_vae(images, encoder_fn, decoder_fn, prior_fn, return_full=False):\n",
    "  \"\"\"Builds the variational auto-encoder and its loss function.\n",
    "  Args:\n",
    "    images: A `int`-like `Tensor` containing observed inputs X. The first\n",
    "      dimension (axis 0) indexes batch elements; all other dimensions index\n",
    "      event elements.\n",
    "    encoder_fn: A callable to build the encoder `q(Z|X)`. This takes a single\n",
    "      argument, a `int`-like `Tensor` representing a batch of inputs `X`, and\n",
    "      returns a Distribution over the batch of latent codes `Z`.\n",
    "    decoder_fn: A callable to build the decoder `p(X|Z)`. This takes a single\n",
    "      argument, a `float`-like `Tensor` representing a batch of latent codes\n",
    "      `Z`, and returns a Distribution over the batch of observations `X`.\n",
    "    prior_fn: A callable to build the prior `p(Z)`. This takes no arguments and\n",
    "      returns a Distribution over a single latent code (\n",
    "    return_full: If True, also return the model components and the encoding.\n",
    "  Returns:\n",
    "    elbo_loss: A scalar `Tensor` computing the negation of the variational\n",
    "      evidence bound (i.e., `elbo_loss >= -log p(X)`).\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(\"encoder\"):\n",
    "    encoder = encoder_fn(images)\n",
    "\n",
    "  with tf.variable_scope(\"prior\"):\n",
    "    prior = prior_fn()\n",
    "\n",
    "  def joint_log_prob(z):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "      decoder = decoder_fn(z)\n",
    "    return decoder.log_prob(images) + prior.log_prob(z)\n",
    "\n",
    "  elbo_loss = tf.reduce_sum(\n",
    "      tfp.vi.monte_carlo_csiszar_f_divergence(\n",
    "          f=tfp.vi.kl_reverse,\n",
    "          p_log_prob=joint_log_prob,\n",
    "          q=encoder,\n",
    "          num_draws=1))\n",
    "  tf.summary.scalar(\"elbo\", elbo_loss)\n",
    "\n",
    "  if return_full:\n",
    "    # Rebuild (and reuse!) the decoder so we can compute stats from it.\n",
    "    encoding_draw = encoder.sample()\n",
    "    with tf.variable_scope(\"decoder\", reuse=True):\n",
    "      decoder = decoder_fn(encoding_draw)\n",
    "    return elbo_loss, encoder, decoder, prior, encoding_draw\n",
    "\n",
    "  return elbo_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/jupyter_client/jsonutil.py:67: DeprecationWarning: Interpreting naive datetime as local 2018-05-22 22:03:21.635699. Please add timezone info to timestamps.\n",
      "  new_obj[k] = extract_dates(v)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-cdfd2915fda7>:113: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From //anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From //anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From //anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/vae/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From //anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/vae/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/vae/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/vae/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From //anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n",
      "//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_probability.python.vi' has no attribute 'monte_carlo_csiszar_f_divergence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cdfd2915fda7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/dataweekends/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-cdfd2915fda7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    127\u001b[0m                                                \u001b[0mmake_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                                                \u001b[0mmake_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                                                return_full=True)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mreconstructed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-761daaa3a575>\u001b[0m in \u001b[0;36mmake_vae\u001b[0;34m(images, encoder_fn, decoder_fn, prior_fn, return_full)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   elbo_loss = tf.reduce_sum(\n\u001b[0;32m---> 41\u001b[0;31m       tfp.vi.monte_carlo_csiszar_f_divergence(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_reverse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mp_log_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoint_log_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_probability.python.vi' has no attribute 'monte_carlo_csiszar_f_divergence'"
     ]
    }
   ],
   "source": [
    "def save_imgs(x, fname):\n",
    "  \"\"\"Helper method to save a grid of images to a PNG file.\n",
    "  Args:\n",
    "    x: A numpy array of shape [n_images, height, width].\n",
    "    fname: The filename to write to (including extension).\n",
    "  \"\"\"\n",
    "  n = x.shape[0]\n",
    "  fig = figure.Figure(figsize=(n, 1), frameon=False)\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "  for i in range(n):\n",
    "    ax = fig.add_subplot(1, n, i+1)\n",
    "    ax.imshow(x[i].squeeze(),\n",
    "              interpolation=\"none\",\n",
    "              cmap=cm.get_cmap(\"binary\"))\n",
    "    ax.axis(\"off\")\n",
    "  canvas.print_figure(fname, format=\"png\")\n",
    "  print(\"saved %s\" % fname)\n",
    "\n",
    "\n",
    "def visualize_training(images_val,\n",
    "                       reconstructed_images_val,\n",
    "                       random_images_val,\n",
    "                       log_dir, prefix, viz_n=10):\n",
    "  \"\"\"Helper method to save images visualizing model reconstructions.\n",
    "  Args:\n",
    "    images_val: Numpy array containing a batch of input images.\n",
    "    reconstructed_images_val: Numpy array giving the expected output\n",
    "      (mean) of the decoder.\n",
    "    random_images_val: Optionally, a Numpy array giving the expected output\n",
    "      (mean) of decoding samples from the prior, or `None`.\n",
    "    log_dir: The directory to write images (Python `str`).\n",
    "    prefix: A specific label for the saved visualizations, which\n",
    "      determines their filenames (Python `str`).\n",
    "    viz_n: The number of images from each batch to visualize (Python `int`).\n",
    "  \"\"\"\n",
    "  save_imgs(images_val[:viz_n],\n",
    "            os.path.join(log_dir, \"{}_inputs.png\".format(prefix)))\n",
    "  save_imgs(reconstructed_images_val[:viz_n],\n",
    "            os.path.join(log_dir,\n",
    "                         \"{}_reconstructions.png\".format(prefix)))\n",
    "\n",
    "  if random_images_val is not None:\n",
    "    save_imgs(random_images_val[:viz_n],\n",
    "              os.path.join(log_dir,\n",
    "                           \"{}_prior_samples.png\".format(prefix)))\n",
    "\n",
    "\n",
    "def build_fake_data(num_examples=10):\n",
    "  \"\"\"Build fake MNIST-style data for unit testing.\"\"\"\n",
    "\n",
    "  class Dummy(object):\n",
    "    pass\n",
    "\n",
    "  num_examples = 10\n",
    "  mnist_data = Dummy()\n",
    "  mnist_data.train = Dummy()\n",
    "  mnist_data.train.images = np.float32(np.random.randn(\n",
    "      num_examples, np.prod(IMAGE_SHAPE)))\n",
    "  mnist_data.train.labels = np.int32(np.random.permutation(\n",
    "      np.arange(num_examples)))\n",
    "  mnist_data.train.num_examples = num_examples\n",
    "  mnist_data.validation = Dummy()\n",
    "  mnist_data.validation.images = np.float32(np.random.randn(\n",
    "      num_examples, np.prod(IMAGE_SHAPE)))\n",
    "  mnist_data.validation.labels = np.int32(np.random.permutation(\n",
    "      np.arange(num_examples)))\n",
    "  mnist_data.validation.num_examples = num_examples\n",
    "  return mnist_data\n",
    "\n",
    "\n",
    "def build_input_pipeline(mnist_data, batch_size, heldout_size):\n",
    "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
    "  # Build an iterator over training batches.\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (mnist_data.train.images, np.int32(mnist_data.train.labels)))\n",
    "  training_batches = training_dataset.repeat().batch(batch_size)\n",
    "  training_iterator = training_batches.make_one_shot_iterator()\n",
    "\n",
    "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
    "  # i.e., return the entire heldout set as a constant.\n",
    "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (mnist_data.validation.images,\n",
    "       np.int32(mnist_data.validation.labels)))\n",
    "  heldout_frozen = (heldout_dataset.take(heldout_size).\n",
    "                    repeat().batch(heldout_size))\n",
    "  heldout_iterator = heldout_frozen.make_one_shot_iterator()\n",
    "\n",
    "  # Combine these into a feedable iterator that can switch between training\n",
    "  # and validation inputs.\n",
    "  handle = tf.placeholder(tf.string, shape=[])\n",
    "  feedable_iterator = tf.data.Iterator.from_string_handle(\n",
    "      handle, training_batches.output_types, training_batches.output_shapes)\n",
    "  images, labels = feedable_iterator.get_next()\n",
    "\n",
    "  return images, labels, handle, training_iterator, heldout_iterator\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "  del argv  # unused\n",
    "  FLAGS.encoder_layers = [int(units) for units\n",
    "                          in FLAGS.encoder_layers.split(\",\")]\n",
    "  FLAGS.decoder_layers = [int(units) for units\n",
    "                          in FLAGS.decoder_layers.split(\",\")]\n",
    "  FLAGS.activation = getattr(tf.nn, FLAGS.activation)\n",
    "  if tf.gfile.Exists(FLAGS.model_dir):\n",
    "    tf.logging.warn(\"Deleting old log directory at {}\".format(FLAGS.model_dir))\n",
    "    tf.gfile.DeleteRecursively(FLAGS.model_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.model_dir)\n",
    "\n",
    "  if FLAGS.fake_data:\n",
    "    mnist_data = build_fake_data()\n",
    "  else:\n",
    "    mnist_data = mnist.read_data_sets(FLAGS.data_dir)\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    (images, _, handle,\n",
    "     training_iterator, heldout_iterator) = build_input_pipeline(\n",
    "         mnist_data, FLAGS.batch_size, mnist_data.validation.num_examples)\n",
    "\n",
    "    # Reshape as a pixel image and dynamically binarize pixels.\n",
    "    images = tf.reshape(images, shape=[-1] + IMAGE_SHAPE)\n",
    "    images = tf.cast(images > 0.5, dtype=tf.int32)\n",
    "\n",
    "    # Build the model and ELBO loss function.\n",
    "    elbo_loss, _, decoder, prior, _ = make_vae(images,\n",
    "                                               make_encoder,\n",
    "                                               make_decoder,\n",
    "                                               make_prior,\n",
    "                                               return_full=True)\n",
    "    reconstructed_images = decoder.mean()\n",
    "\n",
    "    # Decode samples from the prior for visualization.\n",
    "    prior_samples = prior.sample(10)\n",
    "    with tf.variable_scope(\"decoder\", reuse=True):\n",
    "      decoded = make_decoder(prior_samples)\n",
    "      random_images = decoded.mean()\n",
    "\n",
    "    # Perform variational inference by minimizing the -ELBO.\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "    train_op = optimizer.minimize(elbo_loss)\n",
    "\n",
    "    summary = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "      summary_writer = tf.summary.FileWriter(FLAGS.model_dir, sess.graph)\n",
    "      sess.run(init)\n",
    "\n",
    "      # Run the training loop.\n",
    "      train_handle = sess.run(training_iterator.string_handle())\n",
    "      heldout_handle = sess.run(heldout_iterator.string_handle())\n",
    "      for step in range(FLAGS.max_steps):\n",
    "        start_time = time.time()\n",
    "        _, loss_value = sess.run([train_op, elbo_loss],\n",
    "                                 feed_dict={handle: train_handle})\n",
    "        duration = time.time() - start_time\n",
    "        if step % 100 == 0:\n",
    "          print(\"Step: {:>3d} Loss: {:.3f} ({:.3f} sec)\".format(\n",
    "              step, loss_value, duration))\n",
    "\n",
    "          # Update the events file.\n",
    "          summary_str = sess.run(summary, feed_dict={handle: train_handle})\n",
    "          summary_writer.add_summary(summary_str, step)\n",
    "          summary_writer.flush()\n",
    "\n",
    "        # Periodically save a checkpoint and visualize model progress.\n",
    "        if (step + 1) % FLAGS.viz_steps == 0 or (step + 1) == FLAGS.max_steps:\n",
    "          checkpoint_file = os.path.join(FLAGS.model_dir, \"model.ckpt\")\n",
    "          saver.save(sess, checkpoint_file, global_step=step)\n",
    "\n",
    "          # Visualize inputs and model reconstructions from the training set.\n",
    "          images_val, reconstructions_val, random_images_val = sess.run(\n",
    "              (images, reconstructed_images, random_images),\n",
    "              feed_dict={handle: train_handle})\n",
    "          visualize_training(images_val,\n",
    "                             reconstructions_val,\n",
    "                             random_images_val,\n",
    "                             log_dir=FLAGS.model_dir,\n",
    "                             prefix=\"step{:05d}_train\".format(step))\n",
    "\n",
    "          # Visualize inputs and model reconstructions from the validation set.\n",
    "          heldout_images_val, heldout_reconstructions_val = sess.run(\n",
    "              (images, reconstructed_images),\n",
    "              feed_dict={handle: heldout_handle})\n",
    "          visualize_training(heldout_images_val,\n",
    "                             heldout_reconstructions_val,\n",
    "                             None,\n",
    "                             log_dir=FLAGS.model_dir,\n",
    "                             prefix=\"step{:05d}_validation\".format(step))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataweekends",
   "language": "python",
   "name": "dataweekends"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
