{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "### Pros\n",
    "- low level & flexible\n",
    "- portable\n",
    "- auto differentiation\n",
    "- distributed \n",
    "- based on creation of **computational graphs**\n",
    "- can be run on variety of platforms (mobile phones, CPU, GPU, TPU)\n",
    "- good for lots of data, even with SVM, Decision trees, etc.\n",
    "\n",
    "### Cons\n",
    "- not easy to debug\n",
    "\n",
    "## Steps\n",
    "1. define graph\n",
    "2. initialize variables\n",
    "3. run, feeding into placeholders\n",
    "4. evaluate\n",
    "\n",
    "## Components\n",
    "- computational graph\n",
    "- Sessions\n",
    "- Scopes, Names: ways of defining what's in the graph (layers, etc)\n",
    "- Placeholders, Variables, Feed_dict: ways of moving around graph (adding things to graph on the fly, etc)\n",
    "\n",
    "#### Computational graph\n",
    "- Basic unit: OP Gate (multiply two things)\n",
    "- Composition: linear combination (OP gate plus bias)\n",
    "- MatMul, Add, Relu, Xent (cross-entropy) are all nodes in computational graph\n",
    "- Tensor conventions\n",
    "    - fully connected\n",
    "        - Batch_size x Vector length (batch size is always first)\n",
    "        - e.g. [None, 10] means batch size can be defined as an argument when running, and vector length is 10\n",
    "    - Convolutional\n",
    "        - Batch size x H x W x #channels\n",
    "        - e.g. [None ,32,32, 3]\n",
    "        \n",
    "#### Sessions\n",
    "- operations are run in a session\n",
    "\n",
    "#### Distributed\n",
    "- Tensorflow can allocate some operations on CPU and some on GPU\n",
    "- e.g. pre-processing, augmentation and transfer on CPU, training on GPU\n",
    "- Keras does the same thing but you don't have as much of an ability to change it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Record\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/datasets\n",
    "\n",
    "### Visualize graphs\n",
    "https://web.stanford.edu/class/cs20si/2017/lectures/slides_02.pdf\n",
    "\n",
    "### Interactive session\n",
    "- if you don't want to say sess.run() all the time, \n",
    "        sess = tf.InteractiveSession()\n",
    "        x.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data objects\n",
    "### Variables (WEIGHTS)\n",
    "- are things to be learned and updated, e.g. weights and biases\n",
    "- since this is not a Constant, you can actually a.assign(3.5) to it later\n",
    "- so for weights, you can re-assign weights later\n",
    "\n",
    "### Placeholders (INPUTS)\n",
    "- are meant to be fed in onced and used during node execution, e.g. inputs\n",
    "        tf.placeholder(dtype=tf.float32, shape=(2,2))\n",
    "        \n",
    "        or\n",
    "        \n",
    "        x = tf.placeholder(\"float\", [None, n_input])\n",
    "        y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "### Constants\n",
    "- are not used very often for neural networks\n",
    "\n",
    "\n",
    "### Tensors\n",
    "- can reach them \n",
    "        g = tf.get_default_graph()\n",
    "        \n",
    "### Estimator\n",
    "- a tf equivalent of a Keras model \n",
    "- a higher level object representing your model\n",
    "- parts of estimator\n",
    "    - session, \n",
    "    - graph, \n",
    "    - loss, \n",
    "    - optimizer\n",
    "    - feature columns\n",
    "    - initialization\n",
    "    - partitioning\n",
    "    - TrainSpec, EvalSpec\n",
    "- methods of an estimator\n",
    "    1. train()\n",
    "    2. evaluate()\n",
    "    3. predict()\n",
    "    4. export_savedmodel()\n",
    "- requirements of an estimator:\n",
    "    1. input function\n",
    "    2. \n",
    "\n",
    "### tf.data\n",
    "         dataset=tf.data.Dataset.list_files(\"/data/*\")\n",
    "             .map(decode_image)\n",
    "             .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "             .batch(BATCH_SIZE)\n",
    "             \n",
    "- alternative to feed_dict\n",
    "- use this when you can't load dataset into memory\n",
    "- can use this to deploy to multiple GPUs\n",
    "- define a pipeline that generates batches, which you feed into training (below)\n",
    "- this works because you're doing all operations on **pointers** of images, not the images themselves; then loading as needed in batches\n",
    "\n",
    "        ### create dataset\n",
    "        files = tf.data.Dataset.list_files(file_pattern)\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        \n",
    "        ### order of the following 4 lines doesn't matter (will produce different output, though)\n",
    "        dataset = dataset.shuffle(10000)  # element of the dataset is a shard, and you're shuffling shards\n",
    "        dataset = dataset.repeat(NUM_EPOCHS)\n",
    "        dataset = dataset.map(lambda x: tf.parse_single_example(x, features)) # pulls single example from each shard, and here \"features\" shows how to parse a single example (schema)\n",
    "        dataset = dataset.batch(BATCH_SIZE) # batches all single examples, one from each shard\n",
    "        \n",
    "        ### generate iterator of batches\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features = iterator.get_next()  # get one batch\n",
    "        \n",
    "        \n",
    "### Feature columns\n",
    "- can do one-hot\n",
    "        tags = categorical_column_with_vocabulary_list('tags', ['a', 'b', 'c'...])\n",
    "        tags_one_hot = indicator_column(tags)\n",
    "- or embedding\n",
    "- or hashing\n",
    "- etc.\n",
    "        \n",
    "### GPUs\n",
    "- started trying to have GPUs consume from queue, but it was too hard for people\n",
    "- so created dataset api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf.keras\n",
    "- define a model in keras, use tf.keras.estimator.model_to_estimator to convert to tf estimator before fitting (after defining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "L = tf.keras.layers\n",
    "\n",
    "model=tf.keras.Sequential([L.reshape((28,28,1)),\n",
    "                          L.Conv2D(32, 5, activation=t...]\n",
    "                           L.MaxPooling2D(),\n",
    "                           L.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager execution\n",
    "- prototyping mode\n",
    "- allows you to see gradients after batches, do model introspection\n",
    "- once you try different architectures and settle on one, do full training in core mode\n",
    "\n",
    "### When eager mode is activated\n",
    "- when you print a constant, it actually prints the constant and not just the pointer\n",
    "- you can extend the model class to create your own model (see slide before \"Gradient Tape\")\n",
    "- then when you instantiate the model, you can immediately run it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking your work (see slides)\n",
    "- tf.parse_single_example\n",
    "- iterator = dataset.make_one_shot_iterator()\n",
    "- features = iterator.get_next()\n",
    "\n",
    "### Gradient tape\n",
    "- can do backpropagation on demand to check outputs\n",
    "- need this because in eager mode you don't have a graph\n",
    "- so temporarily record gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed deployment\n",
    "\n",
    "https://www.tensorflow.org/deploy/distributed\n",
    "\n",
    "- can do data-parallel (synchronous data parallel)\n",
    "    - split each batch in 4 and send to 4 GPUs\n",
    "    - all do computation and send back losses\n",
    "    - average all losses and update weights\n",
    "- asynchronous data parallel\n",
    "    - GPUs send back updates asynchronously\n",
    "    - more noisy\n",
    "- or with multi-branch models, split parts of models into different GPUsj\n",
    "\n",
    "\n",
    "**Note: ** GPU is most important for deployment/serving/inference, not necessarily for training\n",
    "\n",
    "#### How to switch from GPU to CPU and vice versa:\n",
    "You can also use this to put different parts of the model on CPU and GPU\n",
    "\n",
    "        with tf.device('gpu:0'):\n",
    "            model.compile(loss='binary'...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
