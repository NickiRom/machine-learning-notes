{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rashab and Francesco\n",
    "- deep learning researcher at Stanford working with multiple labs\n",
    "- background in deep learning at Harvard and Stanford\n",
    "- bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes\n",
    "\n",
    "$$ X_t \\dot \\textbf{U} $$\n",
    "(batch, 5) x (5,L) = (batch, L)\n",
    "\n",
    "for Recurrent neural nets, also have $h_{t-1} \\dot W$\n",
    "and W is (L, L) **because all nodes recurrently feed into all other nodes in layer!**\n",
    "and h is (batch, L)\n",
    "\n",
    "\n",
    "if input is (5,1) and layer has 3 nodes and is recurrent, output will have be 3 numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias trick\n",
    "\n",
    "- no difference between bias and weight\n",
    "- bias trick is to input all 1's as an example, learn weights with that example as well\n",
    "- learns information that is input-independent\n",
    "- for example, when object doesn't move in image, the bias will learn to detect the object (or lack thereof) in that location\n",
    "\n",
    "*The bias node/term is there only to ensure the predicted output will be unbiased. If your input has a dynamic (range) that goes from -1 to +1 and your output is simply a translation of the input by +3, a neural net with a bias term will simply have the bias neuron with a non-zero weight while the others will be zero. If you do not have a bias neuron in that situation, all the activation functions and weigh will be optimized so as to mimic at best a simple addition, using sigmoids/tangents and multiplication.*\n",
    "\n",
    "*If both your inputs and outputs have the same range, say from -1 to +1, then the bias term will probably not be useful.*\n",
    "\n",
    "*You could have a look at the weigh of the bias node in the experiment you mention. Either it is very low, and it probably means the inputs and outputs are centered already. Or it is significant, and I would bet that the variance of the other weighs is reduced, leading to a more stable (and less prone to overfitting) neural net.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log loss\n",
    "\n",
    "$$ -\\frac{1}{n} \\sum_{batch} \\sum_{labels} y_{label} ln( \\hat{y} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate parameters\n",
    "\n",
    "- increasing batch size is equivalent to decreasing learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs\n",
    "### What changes between FCN and CNN?\n",
    "\n",
    "#### receptive field\n",
    "\n",
    "#### kernel/filter\n",
    "- looks at relationships between pixels \n",
    "- instead of using one weight per pixel, have as many weights as your filter\n",
    "- e.g. for a 9x9 pixel image, FCN has 81 weights, CNN has 9 weights if you use a 3x3 filter\n",
    "- in practice, weights from the individual nodes are linked, and each node output is a feature map of n numbers (result of convolution)\n",
    "- if you wanted to draw it the way you draw a dense network, a 9x9 image with a 3x3 filter would have 9x9xn nodes in next layer, where n = 9-3+1 = 7 (?)\n",
    "\n",
    "#### Shape of weight tensor\n",
    "$$ (H_F, W_F, C_{input}, C_{output})$$\n",
    "\n",
    "#### pooling\n",
    "- down-sampling to reduce the resolution of the feature maps\n",
    "\n",
    "#### ImageNet\n",
    "- a labeled dataset that has given rise to many pre-trained models, e.g. VGG or ResNet\n",
    "- used to be a benchmark\n",
    "\n",
    "#### Sequence problems\n",
    "- language translation is a many-to-many\n",
    "$$ y_t = f(vx + uy_{t-1})$$\n",
    "\n",
    "- so there are also special weights associated with the previous output that figure into the next prediction\n",
    "\n",
    "#### Attention layers\n",
    "- take in a corpus (imagery or text, e.g.) and a question to be answered\n",
    "- could this be used to answer multiple questions from same image?\n",
    "\n",
    "#### Archictecture tips\n",
    "- Conv, pooling, Relu, Conv, pooling, Relu, ...., Flatten, Dense, outputs\n",
    "- don't put Relu before pooling; it gives the same output as Pool > Relu but is more expensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features from text\n",
    "\n",
    "- have a dictionary where each word is encoded by an index (dense representation of one-hot)\n",
    "- but this doesn't capture relationships between words\n",
    "- instead, use embedding to go from a simple index to a dense vector for each word.  You choose the dimensions, but typical choices are 50-100 weights for each word\n",
    "- so similar words should have similar meanings\n",
    "- and you can have relationships between words, indicated by **parallel vectors**\n",
    "- can put these vectors as first layer in network and learn their weights\n",
    "- so # words * vector length per word is size of first layer\n",
    "- word2vec learns vectors directly by predicting co-occurrences of words in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting set up\n",
    "- https://www.dropbox.com/sh/rptnshr1j0nqmh6/AACbihf2aYQNG6rzH9Wh29eLa?dl=0\n",
    "\n",
    "- conda update conda\n",
    "- conda env create\n",
    "- source activate dataweekends\n",
    "- jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras versus Tensorflow\n",
    "- Keras is high-level API easy for beginners\n",
    "- Keras runs on theano or tensorflow\n",
    "- Keras is less flexible, less verbose\n",
    "- Keras works well for training prod models, but you might not want to use it if:\n",
    "    - serving/hosting on the cloud\n",
    "    - custom loss function\n",
    "    - custom architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "### Motivation\n",
    "- want to learn a new model, but don't have a lot of data\n",
    "- want to make use of features learned for another model\n",
    "\n",
    "### How to do it\n",
    "- if you want to predict same number of classes, don't change anything\n",
    "- imagining features from low-level (early hidden layers) to high-level, how far into the architecture do you think your new problem will diverge from the old problem?\n",
    "- freeze layers you don't want to retrain (and don't learn anything else about them! They will stay the same), except for last softmax layer. Those frozen layers give you bottleneck layers, then have a dense FCN at the end\n",
    "- better to start with a model pre-trained on **many** categories, not just dogs vs cats\n",
    "- un-freeze end layers, or beginning layers, or both\n",
    "- at a minimum, re-train all layers after convolutional/pooling\n",
    "\n",
    "### Image augmentation\n",
    "- instead of image augmentation by rotation, translation, etc.  Can add noise to dense layers to \"augment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving models\n",
    "- Used to be PMML\n",
    "- Now ONNX (pronounced onyx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "### Activation function\n",
    "- Notice that weights w and u don't depend on time\n",
    "- first order filter with nonlinear function on top\n",
    "- kind of like a moving average\n",
    "$$ h_t = tanh(w^1h^1_{t-1} + u^1x_t)$$\n",
    "\n",
    "$$ h_t^2 = tanh(w^2h^2_{t-1} + u^2h^1_t)$$\n",
    "\n",
    "### How to train\n",
    "- if you want to capture seasonality, need to train with sequence at least as long as the period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "- TF lite : for deploying to mobile phones\n",
    "- TF JS: for deploying into browser\n",
    "- TF serving: google's native way of serving models\n",
    "    - has continuous training pipeline\n",
    "    - deploy multiple models\n",
    "    - not easy, so here are some other options:\n",
    "- AWS sagemaker\n",
    "    - hosted jupyter notebook where you define models\n",
    "    - training engine where you launch model for training\n",
    "    - endpoint declaration to create a model API\n",
    "- PipelineAI\n",
    "    - simpler but more experimental (right now) than sagemaker\n",
    "-Floydhub\n",
    "    - deploy by running \"floyd run --mode serve\" in command line\n",
    "    - deploys behind Flask app\n",
    "    - not the most performant but very simple\n",
    "- google cloud ML\n",
    "    - cloud data lab (like jupyter notebok)\n",
    "    - very similar to sagemaker\n",
    "    - train on GCP, deploy to API endpoint\n",
    "- determinedAI\n",
    "    - paid product that manages deployments for you\n",
    "    \n",
    "#### Example using flask and jquery to deploy a model\n",
    "github.com/ghego/tensorflow-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
