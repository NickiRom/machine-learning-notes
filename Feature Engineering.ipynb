{
 "metadata": {
  "name": "",
  "signature": "sha256:3387cc2e4eefdcd57c675fa32016fb600f2a273065d3bc8e97648aa1b46ebd31"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Distributions \n",
      "###Normal Distribution\n",
      "- the simplest and most well-known distribution\n",
      "- X~N(0,1) indicates that the random variable X is normally distributed\n",
      "- this is a special case of the chi-squared distribution with mean of 0 and variance of 1\n",
      "\n",
      "###Chi-Squared Distribution ($\\chi^2$)\n",
      "- A bell curve-shaped distribution\n",
      "- X~$\\chi^2 (\\mu, \\sigma)$ indicates a mean of $\\mu$ and a variance of $\\sigma$\n",
      "\n",
      "###Wishart Distribution\n",
      "- A Normal distribution, generalized to n dimensions\n",
      "- X~N(0, $\\Sigma$) indicates X is normally distributed in n dimensions with an $n \\times n$ co-variance matrix of $\\Sigma$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Covariance matrices\n",
      "\n",
      "\\begin{equation*}\n",
      "cov(X,Y) = \\mathbb{E} \\left( [ X-\\mathbb{E}(X) ][ Y-\\mathbb{E}(Y) ] \\right)\n",
      "\\end{equation*}\n",
      "\n",
      "- or in geometric terms, the average square representing the difference between expectation and reality in the x and y directions\n",
      "- correlation is symmetric, a unit-scaled version of covariance\n",
      "\n",
      "\\begin{equation*}\n",
      "corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}\n",
      "\\end{equation*}\n",
      "\n",
      "\n",
      "\n",
      "- Covariance tells us how the data's variance in one feature changes wrt another\n",
      "\n",
      "<img src=\"covariance.png\" width=\"500\" height=\"300\">\n",
      "\n",
      "###Eigenvectors and Eigenvalues\n",
      "- Covariance matrices $\\sum$ express the spread and orientation of the data wrt two features at a time\n",
      "- the vector and magnitude of this spread are the eigenvector and eigenvalue\n",
      "\n",
      "<img src=\"eigenvectors.png\" width=\"250\" height=\"150\">\n",
      "\n",
      "   - project the data D onto a vector $\\vec{v}$:     $\\vec{v}^TD$\n",
      "   - evaluate the variance:\n",
      "   \\begin{equation*}\n",
      "   \\sigma^2 = \\vec{v}^T \\sum \\vec{v}\n",
      "   \\end{equation*}\n",
      "   - find the vector $\\vec{v}$ such that $\\vec{v}^T \\sum \\vec{v}$ (the variance) is maximized\n",
      "   - this is equivalent to the largest eigenvector of matrix $\\sum$\n",
      "   - if $\\sum$ is a diagonal matrix (covariances are 0), the variances are equal to the eigenvalues $\\lambda$\n",
      "\n",
      "    ####Singular Value Decomposition\n",
      "    - a method to get a diagonal covariance matrix using eigendecomposition\n",
      "    - the resulting eigenvectors point in the directions of largest variance\n",
      "    - eigenvalues represent the magnitude of variance in these directions\n",
      "    - [more info on SVD](http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)\n",
      "    \n",
      "###[Scikit-Learn implementation](http://scikit-learn.org/stable/modules/covariance.html)\n",
      "\n",
      "###Empirical covariance\n",
      "- estimated using Maximum Likelihood Estimator\n",
      "- only if n >> features\n",
      "- [how to fit a covariance object to data](http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html#example-covariance-plot-covariance-estimation-py)\n",
      "\n",
      "        sklearn.covariance.empirical_covariance(X, assume_centered=False)\n",
      "        \n",
      "***note:*** This is not a good estimation of eigenvalues of the covariance matrix\n",
      "\n",
      "###Shrinkage covariance\n",
      "- MLE covariance may be unbiased, but it is not a good estimator of eigenvalues\n",
      "- and if n < features, the covariance matrix is not invertable (and the precision matrix is therefore unobtainable)\n",
      "- given a shrinkage parameter $\\alpha$ that amounts to a bias/variance tradeoff, compute an invertible matrix that reduces the ratio between the smallest and largest eigenvalues of $\\sum$\n",
      "- shrinkage parameter $\\alpha$ should be chosen by cross-validation\n",
      "\n",
      "#    $\\sum_{shrunk} = (1-\\alpha)\\hat\\Sigma+ \\alpha \\frac{Tr\\hat\\Sigma}{p}\\mathbf{I}d$\n",
      "    \n",
      "- The optimal $\\alpha$ can be computed using the [Ledoit-Wolf shrinkage algorithm](http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html#example-covariance-plot-covariance-estimation-py)\n",
      "\n",
      "###[Sparse Inverse covariance](http://scikit-learn.org/stable/auto_examples/covariance/plot_sparse_cov.html)\n",
      "- use GraphLasso (an L1 regularization parameter) to sparsify the covariance matrix such that it can be inverted to produce a precision matrix\n",
      "\n",
      "**Tip: ** Use a correlation matrix (standardized observations) \n",
      "\n",
      "###Robust covariance estimation\n",
      "- Find the minimum covariance determinant\n",
      "- in simple terms, find the proportion of non-outlier observations and compute their empirical covariance matrix\n",
      "- then re-scale to represent entire dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#$ \\omega_B = \\begin{pmatrix}\\omega_1 \\\\ \\omega_2 \\\\ \\vdots \\\\ \\omega_n \\end{pmatrix} $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#\\begin{equation*}\n",
      "\\sigma^2_B = \\omega^{\\prime}_B \\Sigma \\omega_B = \\begin{pmatrix}\\omega_1 \\ \\omega_2 \\ \\cdots \\ \\omega_n \\end{pmatrix} \\begin{pmatrix} \\sigma^2_{1} & \\sigma_{1,2} & \\cdots & \\sigma_{1,n} \\\\ \\sigma_{2,1} & \\sigma^2_{2} & \\cdots & \\sigma_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{n,1} & \\sigma_{n,2} & \\cdots & \\sigma^2_{n} \\end{pmatrix} \\begin{pmatrix}\\omega_1 \\\\ \\omega_2 \\\\ \\cdots \\\\ \\omega_n \\end{pmatrix}\n",
      "\\end{equation*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#\\begin{equation*}\n",
      "\\sigma^2_P = \\omega^{\\prime}_P \\Sigma \\omega_P = \\begin{pmatrix}\\omega_1 \\ \\omega_2 \\ \\cdots \\ \\omega_n \\end{pmatrix} \\begin{pmatrix} \\sigma^2_{1} & \\sigma_{1,2} & \\cdots & \\sigma_{1,n} \\\\ \\sigma_{2,1} & \\sigma^2_{2} & \\cdots & \\sigma_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{n,1} & \\sigma_{n,2} & \\cdots & \\sigma^2_{n} \\end{pmatrix} \\begin{pmatrix}\\omega_1 \\\\ \\omega_2 \\\\ \\cdots \\\\ \\omega_n \\end{pmatrix}\n",
      "\\end{equation*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#$$ Cov(x_1,x_2) = \\sigma_{12} = \\frac{ \\sum^{n}\\left( x^i_{1}-\\mu_1 \\right)~ \\sum^n \\left( x^i_{2}-\\mu_2 \\right)}{N} $$\n",
      "\n",
      "#$ Correlation = \\frac{cov(x_1,x_2)}{\\sigma_{x_1} \\sigma_{x_2} } $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#$ \\sigma^2 = \\frac{\\Sigma^n_i (x_i-\\mu)^2}{N}$\n",
      "\n",
      "#$ Variance = \\sigma^2 = \\frac{\\Sigma^n_i (x_i-\\mu)^2}{N}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#\\begin{equation*} \\Sigma = \\begin{pmatrix} \\sigma^2_{1} & \\sigma_{1,2} & \\cdots & \\sigma_{1,n} \\\\ \\sigma_{2,1} & \\sigma^2_{2} & \\cdots & \\sigma_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{n,1} & \\sigma_{n,2} & \\cdots & \\sigma^2_{n} \\end{pmatrix} \\end{equation*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#$$ Cov(x,y) = \\frac{ \\sum^{n}\\left( x^i-\\mu_x \\right)~ \\sum^n \\left( x^i-\\mu_y \\right)}{N} $$\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}