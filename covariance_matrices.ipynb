{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Distributions \n",
    "\n",
    "An intuitive motivation for the covariance matrix\n",
    "\n",
    "###Normal Distribution\n",
    "- the simplest and most well-known distribution\n",
    "- X~N(0,1) indicates that the random variable X is normally distributed\n",
    "- this is a special case of the chi-squared distribution with mean of 0 and variance of 1\n",
    "\n",
    "###Chi-Squared Distribution ($\\chi^2$)\n",
    "- A bell curve-shaped distribution\n",
    "- X~$\\chi^2 (\\mu, \\sigma)$ indicates a mean of $\\mu$ and a variance of $\\sigma$\n",
    "\n",
    "###Wishart Distribution\n",
    "- A Normal distribution, generalized to n dimensions\n",
    "- X~N(0, $\\Sigma$) indicates X is normally distributed in n dimensions with an $n \\times n$ co-variance matrix of $\\Sigma$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Covariance matrices\n",
    "\n",
    "\\begin{equation*}\n",
    "cov(X,Y) = \\mathbb{E} \\left( [ X-\\mathbb{E}(X) ][ Y-\\mathbb{E}(Y) ] \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "To represent the pairwise covariance of n items, we use a covariance matrix:\n",
    "\n",
    "###\\begin{equation*} \\Sigma = \\begin{pmatrix} \\sigma^2_{1} & \\sigma_{1,2} & \\cdots & \\sigma_{1,n} \\\\ \\sigma_{2,1} & \\sigma^2_{2} & \\cdots & \\sigma_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{n,1} & \\sigma_{n,2} & \\cdots & \\sigma^2_{n} \\end{pmatrix} \\end{equation*}###\n",
    "\n",
    "- or in geometric terms, the average square representing the difference between expectation and reality in the x and y directions\n",
    "- correlation is symmetric, a unit-scaled version of covariance\n",
    "\n",
    "\\begin{equation*}\n",
    "corr(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}\n",
    "\\end{equation*}\n",
    "\n",
    "- Covariance tells us how the data's variance in one feature changes wrt another\n",
    "\n",
    "<img src=\"images/covariance.png\" width=\"500\" height=\"300\">\n",
    "\n",
    "###Eigenvectors and Eigenvalues\n",
    "- Covariance matrices $\\sum$ express the spread and orientation of the data wrt two features at a time\n",
    "- the vector and magnitude of this spread are the eigenvector and eigenvalue\n",
    "\n",
    "<img src=\"images/eigenvectors.png\" width=\"250\" height=\"150\">\n",
    "\n",
    "   - project the data D onto a vector $\\vec{v}$:     $\\vec{v}^TD$\n",
    "   - evaluate the variance:\n",
    "   $\\sigma^2 = \\vec{v}^T \\sum \\vec{v}$\n",
    "   - find the vector $\\vec{v}$ such that $\\vec{v}^T \\sum \\vec{v}$ (the variance) is maximized\n",
    "   - this is equivalent to the largest eigenvector of matrix $\\sum$\n",
    "   - if $\\sum$ is a diagonal matrix (covariances are 0), the variances are equal to the eigenvalues $\\lambda$\n",
    "\n",
    "    ####Singular Value Decomposition\n",
    "    - a method to get a diagonal covariance matrix using eigendecomposition\n",
    "    - the resulting eigenvectors point in the directions of largest variance\n",
    "    - eigenvalues represent the magnitude of variance in these directions\n",
    "    - [more info on SVD](http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)\n",
    "    \n",
    "###[Scikit-Learn implementation](http://scikit-learn.org/stable/modules/covariance.html)\n",
    "\n",
    "###Empirical covariance\n",
    "- estimated using Maximum Likelihood Estimator\n",
    "- only if n >> features\n",
    "- [how to fit a covariance object to data](http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html#example-covariance-plot-covariance-estimation-py)\n",
    "\n",
    "        sklearn.covariance.empirical_covariance(X, assume_centered=False)\n",
    "        \n",
    "***note:*** This is not a good estimation of eigenvalues of the covariance matrix\n",
    "\n",
    "###Shrinkage covariance\n",
    "- MLE covariance may be unbiased, but it is not a good estimator of eigenvalues\n",
    "- and if n < features, the covariance matrix is not invertable (and the precision matrix is therefore unobtainable)\n",
    "- given a shrinkage parameter $\\alpha$ that amounts to a bias/variance tradeoff, compute an invertible matrix that reduces the ratio between the smallest and largest eigenvalues of $\\sum$\n",
    "- shrinkage parameter $\\alpha$ should be chosen by cross-validation\n",
    "\n",
    "##    $\\sum_{shrunk} = (1-\\alpha)\\hat\\Sigma+ \\alpha \\frac{Tr\\hat\\Sigma}{p}\\mathbf{I}d$\n",
    "    \n",
    "- The optimal $\\alpha$ can be computed using the [Ledoit-Wolf shrinkage algorithm](http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html#example-covariance-plot-covariance-estimation-py)\n",
    "\n",
    "###[Sparse Inverse covariance](http://scikit-learn.org/stable/auto_examples/covariance/plot_sparse_cov.html)\n",
    "- use GraphLasso (an L1 regularization parameter) to sparsify the covariance matrix such that it can be inverted to produce a precision matrix\n",
    "\n",
    "**Tip: ** Use a correlation matrix (standardized observations) \n",
    "\n",
    "###Robust covariance estimation\n",
    "- Find the minimum covariance determinant\n",
    "- in simple terms, find the proportion of non-outlier observations and compute their empirical covariance matrix\n",
    "- then re-scale to represent entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Covariance matrices in portfolio theory\n",
    "\n",
    "The relative weighting of N investments can be represented as a vector:\n",
    "\n",
    "$ \\omega_B = \\begin{pmatrix}\\omega_1 \\\\ \\omega_2 \\\\ \\vdots \\\\ \\omega_n \\end{pmatrix} $\n",
    "\n",
    "And the covariance matrix of a portfolio can therefore be calculated by:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma^2_P = \\omega^{\\prime}_P \\Sigma \\omega_P = \\begin{pmatrix}\\omega_1 \\ \\omega_2 \\ \\cdots \\ \\omega_n \\end{pmatrix} \\begin{pmatrix} \\sigma^2_{1} & \\sigma_{1,2} & \\cdots & \\sigma_{1,n} \\\\ \\sigma_{2,1} & \\sigma^2_{2} & \\cdots & \\sigma_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{n,1} & \\sigma_{n,2} & \\cdots & \\sigma^2_{n} \\end{pmatrix} \\begin{pmatrix}\\omega_1 \\\\ \\omega_2 \\\\ \\cdots \\\\ \\omega_n \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "To identify the pairwise covariance matrix representing the interaction between pairs of investments, one can record the % return on investment after a short, pre-determined period of time.  For example, historical returns can be measured for every month in the past year.  The variance $\\sigma^2$ of an investment's monthly return is calculated as:\n",
    "\n",
    "####$ Variance = \\sigma^2 = \\frac{\\Sigma^n_i (x_i-\\mu)^2}{N}$####\n",
    "\n",
    "where $\\mu$ represents the average % return on investment over t trials (12 months, in our example).\n",
    "\n",
    "\n",
    "Similarly, the covariance of two investments can be calculated as follows:\n",
    "\n",
    "####$$ Cov(x_1,x_2) = \\sigma_{12} = \\frac{ \\sum^{n}\\left( x^i_{1}-\\mu_1 \\right)~ \\sum^n \\left( x^i_{2}-\\mu_2 \\right)}{N} $$####\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
