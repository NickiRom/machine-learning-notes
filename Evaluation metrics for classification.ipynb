{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluation of Classification Models\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$$$\n",
    "###Evaluation outputs\n",
    "___\n",
    "[**ROC curve**](#ROC)\n",
    "\n",
    "[**Lift chart**](#Lift Chart)\n",
    "\n",
    "[**Calibration plot**](#Calibration)\n",
    "\n",
    "[**Confusion Matrix**](#Confusion Matrix)\n",
    "$$$$\n",
    "###Metrics\n",
    "___\n",
    "\n",
    "[**Classification Accuracy**](#Classification Accuracy): What % of predictions were correct?\n",
    "\n",
    "[**Precision**](#Precision): How pure is our pool of predicted Positives?\n",
    "\n",
    "[**Recall**](#Recall):  How often do we identify True Positives?\n",
    "\n",
    "[**F-Measure or F1 Score**](#F-measure)\n",
    "\n",
    "**Sensitivity:** See Recall\n",
    "\n",
    "[**Specificity**](#Specificity): How well do we identify True Negatives?\n",
    "\n",
    "[**Brier Score:**](#Brier Score) How well calibrated is the classifier?\n",
    "$$$$\n",
    "###Caveats\n",
    "___\n",
    "[**Accuracy Paradox**](#Accuracy Paradox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ROC'></a>\n",
    "##ROC curve\n",
    "\n",
    "- independent of P:N ratio (good for comparing classifiers with different ratios)\n",
    "- each point represents a different classifier\n",
    "- to get a classifier on a line between two points, use an ensemble method: choose classifier A's prediction with a probability &alpha; and classifier B prediction with probability 1 - &alpha;\n",
    " \n",
    "\\begin{equation*}\n",
    "x = FPrate(t), y = TPrate(t)\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary classifiers** give us a point on the ROC graph\n",
    "\n",
    "**Probabilistic classifiers** give us a curve by varying the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¿¼<img src=\"ROC.png\" width=\"300\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "A_{ROC} =\\int_{0}^{1}\\frac{\\mathbf{TP}}{P}d{\\frac{\\mathbf{FP}}{N}} = \\frac{1}{PN} \\int_0^N {TP} d{FP}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "A_{ROC1} = P(\\mbox{ random positive example } > \\mbox{ random negative example})\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "A_{ROC2} = P(\\mbox{ random P } > \\mbox{ random N }) + \\frac{1}{2}P(\\mbox{random P } > \\mbox{random N})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ROC1_vs_2.png\" width=\"300\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Classification Accuracy'></a>\n",
    "####Optimal classification accuracy####\n",
    "\n",
    "**Q:** What percent of predictions were correct?  (TP or TN)\n",
    "\n",
    "\\begin{equation*}\n",
    "Accuracy = \\frac{\\mathbf{TP}+\\mathbf{TN}}{P+N}\n",
    "\\end{equation*}\n",
    "\n",
    "    So the equation for iso-performance or iso-parametric accuracy is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{TPrate} = \\frac{N}{P}\\mathbf{FPrate} + \\frac{Accuracy \\times (P+N)-N}{P}\n",
    "\\end{equation*}\n",
    "\n",
    "    where the slope is equal to the N:P ratio\n",
    "    \n",
    "- So the point of optimal accuracy is where the iso-performance line meets the ROC curve (upper leftmost point of ROC curve)\n",
    "- Therefore, two curves with the same AUC can differ greatly in Accuracy\n",
    "\n",
    "<a id='Accuracy Paradox'></a>\n",
    "*note: the * **Accuracy Paradox** says that you can \"increase accuracy\" even when you're decreasing a classifier's predictive power. \n",
    "    - If TP < FP: you can \"increase accuracy\" by classifying everything as Negative \n",
    "    - If TN < FN: you can \"increase accuracy\" by classifying everything as Positive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Lift Chart'></a>\n",
    "##Lift Chart\n",
    "**Question answered:** If I predict a success, how likely is it that this is actually a success?\n",
    "- Plots TP against all predicted P\n",
    "- If chart follows diagonal, only half of predicted successes are actual successes\n",
    "- unlike ROC, depends on ratio of P:N\n",
    "    - This means that different samples could yield different Lift Charts with identical classification properties\n",
    "- useful when P (and therefore $TPrate$, ROC curve) is unknown\n",
    "- e.g. when serving ads, we don't know the total population of converters P\n",
    "\n",
    "\\begin{equation*}\n",
    "x = Yrate(t) = \\frac{\\mathbf{TP}(t)+\\mathbf{FP}(t)}{P+N}\n",
    "\\end{equation*}\n",
    "\n",
    "where $Yrate$ is essentially the predicted rate of success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LiftChart.png\" width = \"300\" height = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####AUC for Lift Charts\n",
    "- Random classifier will have an AUC of $\\frac{P}{2}$, while a perfect classifier has an AUC of $P$\n",
    "- Not meant to be used for optimal classification\n",
    "- but can find point of maximal profit (related to weighted optimal classification)\n",
    "\n",
    "    ####Profit\n",
    "    - fixed benefit for every correct classification\n",
    "    - reduced by fixed cost for every misclassification\n",
    "    - optimal profit is where expected benefit = expected cost\n",
    "    - adding weights to positive and negative errors impacts the slope of the iso-performance lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Calibration'></a>\n",
    "##Calibration Plot\n",
    "- Re-scales score allocation to reflect actual probabilities\n",
    "- For Probabilistic Classifiers\n",
    "- Dependant on P:N ratio\n",
    "- Calibration is a measure of whether the predicted success rate = the actual success rate\n",
    "\n",
    "**Calibration does not:** affect ROC or Lift Chart\n",
    "\n",
    "**Calibration does:** re-distribute the distribution of probability scores to align with actual probability priors\n",
    "\n",
    "<img src=\"Calibration.png\" width=\"300\" height=\"500\">\n",
    "\n",
    "####In a Calibrated model, we expect...\n",
    "- Of all examples with a probability score of 0.7, 70% are actually successes\n",
    "- The highest theoretical score is a success close to 100% of the time\n",
    "- The ranking order of examples is not changed, but their absolute probability scores change\n",
    "- The model is unbiased (imperative for unbalanced data)\n",
    "\n",
    "####Steps (for SVM) when the mapping function is known\n",
    "1. Choose a subset of examples with the same probabilistic score (or bin samples with similar scores)\n",
    "2. The ratio of P:N in each subset is the true probability\n",
    "3. Since the relationship between SVM scores s(x) and actual probability P(c|x) is often sigmoidal, fit the score distribution to the following function:\n",
    "\n",
    "    $\\hat{P}(c|x) = \\frac{1}{1+e^{As(x)+B}}$\n",
    "\n",
    "4. Find parameters A and B, thereby minimizing the Negative Log-Likelihood\n",
    "5. Transform scores to calibrated curve\n",
    "\n",
    "####Steps when the mapping function is unknown\n",
    "1. Sort training examples according to scores and divide into $b$ bins of equal size (number of bins should be chosen by cross-validation)\n",
    "2. For each bin, find the lower and upper boundadry scores\n",
    "3. For each bin, record the actual proportion of training examples that are successes\n",
    "4. Use this proportion to estimate the corrected probability score\n",
    "\n",
    "*note: this does not work well for small or unbalanced datasets*\n",
    "\n",
    "####If all else fails\n",
    "- use isotonic regression to learn mapping of predictions to actual probabilities\n",
    "- one algorithmic example is Pair-adjacent violators (PAV)\n",
    "- sklearn has an implementation of isotonic regression: http://scikit-learn.org/stable/modules/calibration.html\n",
    "\n",
    "<a id='Brier Score'></a>\n",
    "**Brier Score:** How well calibrated is the classifier? \n",
    "\\begin{equation*}\n",
    "BS = \\frac{1}{N} \\sum_N^{t=1}(f_t-o_t)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Theory:\n",
    "Bianca Zadrozny, Charles Elkan @IBM: http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
    "\n",
    "Miha Vuk, Tomaz Curk (2006) Metodoloski zvezki: http://www.stat-d.si/mz/mz3.1/vuk.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Confusion Matrix'></a>\n",
    "###Confusion Matrix\n",
    "\n",
    "<a id='Precision'></a>\n",
    "**Precision:** How many Positive Predictions are correct?  \n",
    "\\begin{equation*}\n",
    "Precision = \\frac{\\mathbf{TP}}{\\mathbf{TP}+FP}\n",
    "\\end{equation*}\n",
    "\n",
    "<a id='Recall'></a>\n",
    "**Recall aka Sensitivity:** How often do we identify True Positives?\n",
    "\\begin{equation*}\n",
    "Precision = \\frac{\\mathbf{TP}}{\\mathbf{TP}+FN}\n",
    "\\end{equation*}\n",
    "\n",
    "<a id='F-measure'></a>\n",
    "**F-measure:** Combines Precision and Recall\n",
    "- the harmonic mean of Precision and Recall\n",
    "- called F1 Score when $Cost_{precision} = Cost_{recall}$\n",
    "\n",
    "\\begin{equation*}\n",
    "F = 2\\times\\frac{Precision \\times Recall}{Precision + Recall}\n",
    "\\end{equation*}\n",
    "\n",
    "<a id='Specificity'></a>\n",
    "**Specificity:** How often do we identify True Negatives?\n",
    "\n",
    "\\begin{equation*}\n",
    "Specificity = \\frac{\\mathbf{TN}}{\\mathbf{TN}+FP}\n",
    "\\end{equation*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
